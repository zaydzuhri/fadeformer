{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "sdtDsu1Y0EqL"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANr5dn7W0EqR",
        "outputId": "a00cbe8d-e1c6-454b-b552-4ffd17ce17e5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "length of dataset in characters:  493412\n"
          ]
        }
      ],
      "source": [
        "with open('data/kon.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "print(\"length of dataset in characters: \", len(text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pANiObIZ0EqU",
        "outputId": "98f70469-1c89-4341-89e8-c142a14331b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Ui:\n",
            "Sis, come on. You'd better get out of bed. Sis?\n",
            "\n",
            "Yui:\n",
            "Ah! I-it's eight! I'm late! Oh!\n",
            "\n",
            "Ui:\n",
            "Hey, why the rush? Hm?\n",
            "\n",
            "Yui:\n",
            "See you later!\n",
            "\n",
            "Lady:\n",
            "Oh, good morning, Yui.\n",
            "\n",
            "Yui:\n",
            "Good morning!\n",
            "\n",
            "Yui:\n",
            "What?! I read the clock wrong!\n",
            "Starting today, I'm a high schooler!\n",
            "\n",
            "Opening Song\n",
            "Cagayake!GIRLS by 放課後ティータイム(After School Tea Time)\n",
            "\n",
            "Girls:\n",
            "Congratulations on starting school here!\n",
            "\n",
            "Girl 1:\n",
            "Please join the Tennis Club!\n",
            "\n",
            "Girl 2:\n",
            "The Judo Club's better!\n",
            "\n",
            "Girl 3:\n",
            "Please join the Tea Ceremony Club!\n",
            "\n",
            "Girl 4:\n"
          ]
        }
      ],
      "source": [
        "print(text[:500])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "WvM5h6_i3KsM"
      },
      "outputs": [],
      "source": [
        "# remove japanese characters\n",
        "text = ''.join(filter(lambda character:ord(character) < 0x3000, text))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U7SOcWJM0EqW",
        "outputId": "a82a4da8-a8ed-47bf-cfb5-2e4c59dee2cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "unique characters: 93 \n",
            " !\"#$%&'(),-./0123456789:;?ABCDEFGHIJKLMNOPQRSTUVWXYZ[]abcdefghijklmnopqrstuvwxyz{|}~°éū‘’…♪\n"
          ]
        }
      ],
      "source": [
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "print(\"unique characters:\", vocab_size, ''.join(chars))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yobmmaeK0EqX",
        "outputId": "26669f38-4b26-4b53-f642-ee7543e7c578"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "encoded: [48, 64, 25, 0, 46, 64, 74, 11, 1, 58, 70, 68, 60, 1, 70, 69, 13, 1, 52, 70]\n",
            "decoded: Ui:\n",
            "Sis, come on. Yo\n"
          ]
        }
      ],
      "source": [
        "# Very simple tokenizer\n",
        "stoi = {ch:i for i,ch in enumerate(chars)}\n",
        "itos = {i:ch for i,ch in enumerate(chars)}\n",
        "encode = lambda s: [stoi[ch] for ch in s]\n",
        "decode = lambda l: ''.join([itos[i] for i in l])\n",
        "\n",
        "print(\"encoded:\", encode(text[:20]))\n",
        "print(\"decoded:\", decode(encode(text[:20])))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Pnf9KfP0EqY",
        "outputId": "ff581168-335a-4f0f-efeb-0d4bd5b225ca"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "torch.Size([493171])"
            ]
          },
          "execution_count": 53,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data = torch.tensor(encode(text), dtype=torch.int64)\n",
        "data.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lZ2fY1pR0EqY",
        "outputId": "5eb599e0-fb79-4cdb-c4b9-52a781d67151"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([48, 64, 25,  0, 46, 64, 74, 11,  1, 58, 70, 68, 60,  1, 70, 69, 13,  1,\n",
              "        52, 70, 76,  8, 59,  1, 57, 60, 75, 75, 60, 73,  1, 62, 60, 75,  1, 70,\n",
              "        76, 75,  1, 70, 61,  1, 57, 60, 59, 13,  1, 46, 64, 74, 27,  0,  0, 52,\n",
              "        76, 64, 25,  0, 28, 63,  2,  1, 36, 12, 64, 75,  8, 74,  1, 60, 64, 62,\n",
              "        63, 75,  2,  1, 36,  8, 68,  1, 67, 56, 75, 60,  2,  1, 42, 63,  2,  0,\n",
              "         0, 48, 64, 25,  0, 35, 60, 80, 11,  1])"
            ]
          },
          "execution_count": 54,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "data[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oIaYesPh0Eqa",
        "outputId": "caa27cbb-5ae3-4406-8194-646264d4ba99"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([468512]) torch.Size([24659])\n"
          ]
        }
      ],
      "source": [
        "n = int(len(data) * 0.95)\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "print(train_data.shape, val_data.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4bFhizcI0Eqa",
        "outputId": "c93a4d43-6fb2-4de7-aefc-8fed47e4a861"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([48, 64, 25,  0, 46, 64, 74, 11,  1])"
            ]
          },
          "execution_count": 56,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "block_size = 8\n",
        "train_data[:block_size+1]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1VHHMHja0Eqb",
        "outputId": "6a88e780-075f-4c14-e198-9c14d6ae4044"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "context: [48] target: 64\n",
            "context: [48, 64] target: 25\n",
            "context: [48, 64, 25] target: 0\n",
            "context: [48, 64, 25, 0] target: 46\n",
            "context: [48, 64, 25, 0, 46] target: 64\n",
            "context: [48, 64, 25, 0, 46, 64] target: 74\n",
            "context: [48, 64, 25, 0, 46, 64, 74] target: 11\n",
            "context: [48, 64, 25, 0, 46, 64, 74, 11] target: 1\n"
          ]
        }
      ],
      "source": [
        "# context and target simulation\n",
        "x = train_data[:block_size]\n",
        "y = train_data[1:block_size+1]\n",
        "for t in range(block_size):\n",
        "    context = x[:t+1].tolist()\n",
        "    target = y[t].item()\n",
        "    print('context:', context, 'target:', target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "skFCPvQC0Eqc",
        "outputId": "08990c5b-88af-4a51-ce37-3c59989d6d74"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs:\n",
            "torch.Size([4, 8])\n",
            "tensor([[73, 60, 56, 67, 67, 80,  1, 59],\n",
            "        [63, 64, 74, 27,  2,  0, 40, 76],\n",
            "        [36,  1, 56, 73, 60,  1, 62, 70],\n",
            "        [45, 64, 75, 74, 76, 25,  0, 46]])\n",
            "targets:\n",
            "torch.Size([4, 8])\n",
            "tensor([[60, 56, 67, 67, 80,  1, 59, 70],\n",
            "        [64, 74, 27,  2,  0, 40, 76, 62],\n",
            "        [ 1, 56, 73, 60,  1, 62, 70, 64],\n",
            "        [64, 75, 74, 76, 25,  0, 46, 63]])\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(69)\n",
        "batch_size = 4 # number of parallel blocks\n",
        "block_size = 8 # number of characters in each block = context length\n",
        "\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    return x, y\n",
        "\n",
        "xb, yb = get_batch('train')\n",
        "print('inputs:')\n",
        "print(xb.shape)\n",
        "print(xb)\n",
        "print('targets:')\n",
        "print(yb.shape)\n",
        "print(yb)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LZUYR7UC0Eqe",
        "outputId": "68284d87-c596-46d2-b344-2a75d306235f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([32, 93])\n",
            "tensor(5.0349, grad_fn=<NllLossBackward0>)\n",
            "\n",
            "‘RreWQQMv♪a[GMhW6~\n",
            "6n°3fG'O-?.’\"deN‘Jvw°[bD’:Vo9ukRr2{0DGEc|x (%$RgGBspJb#‘, K5P84N.fm-3KAH{:5°G|~&0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "torch.manual_seed(69)\n",
        "\n",
        "class BigramLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        # Bigram language model: single layer, single token prediction\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        logits = self.token_embedding_table(idx)  # (B,T,C), B: batch=4, T: sequence=8, C: vocab=147\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            # flatten the logits and targets for torch cross entropy\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # generate max_new_tokens new tokens given the initial context idx\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :]  # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1)  # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1)  # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1)  # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "\n",
        "m = BigramLanguageModel(vocab_size)\n",
        "logits, loss = m(xb, yb)\n",
        "print(logits.shape)\n",
        "print(loss)\n",
        "\n",
        "# randomly generate 100 tokens from initial model weights and idx = 0 = \\n\n",
        "print(decode(m.generate(idx=torch.zeros(\n",
        "    (1, 1), dtype=torch.long), max_new_tokens=100)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gf1W0gCJ0Eqi",
        "outputId": "149b3a0e-ce3c-42e9-d02e-a77d57a116cc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.4911692142486572\n"
          ]
        }
      ],
      "source": [
        "# training!\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "batch_size = 32\n",
        "for steps in range(10000):  # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    # backprop\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6jVyXqMZ0Eqj",
        "outputId": "2fdc81f8-5de0-40ed-afef-7ec89f467616"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "[Say?\n",
            "Hat Whan.\n",
            "Azu:\n",
            "Mig$we I I'recld h, s(9f(rgha:\n",
            "Misngol so:\n",
            "\n",
            "Sofom cka Iti:\n",
            "I'tg am91't m aw! ught I jus ing than ad..\n",
            "HDodeare.\n",
            "Spr ctinchepa u:\n",
            "Anthearet wha.\n",
            "\n",
            "Wou:\n",
            "Azudangicou:\n",
            "Ohito tawit s, giondnet g thiomachth, °abedo:\n",
            "Ale's he)D'swe wh!\n",
            "Ohe, m I douthhe b'/%Jutari:\n",
            "Tsthoulestoitheari:\n",
            "Yu\n"
          ]
        }
      ],
      "source": [
        "# generate 100 tokens from trained model weights and idx = 0 = \\n\n",
        "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mW7SE2pj-muH"
      },
      "source": [
        "Lets try out lower dimensional embeddings + positional embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "id": "wRzgeS0z-i_A"
      },
      "outputs": [],
      "source": [
        "class BigramEmbedLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super().__init__()\n",
        "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        # embed block sized context length as positional embeddings of the same size\n",
        "        self.position_embedding_table = nn.Embedding(block_size, embed_size)\n",
        "        # Language Modelling (?) Head is a standard linear layer to go from \n",
        "        # embeddings back to logits of vocab_size\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T)) # (T,C) [0...T-1]\n",
        "        x = tok_embd + pos_embd\n",
        "        logits = self.lm_head(x) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            #crop idx to the last block_size tokens\n",
        "            idx_context = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_context)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NIhvnIhaAbKC",
        "outputId": "cd1a69f4-192d-4b94-d9a6-bc4d9246caa0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "2.5127875804901123\n"
          ]
        }
      ],
      "source": [
        "# training!\n",
        "m = BigramEmbedLanguageModel(vocab_size, 32)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "batch_size = 32\n",
        "for steps in range(10000):  # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    # backprop\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QQzNB-QIBUOJ",
        "outputId": "6bcda34d-2bf7-49ab-a0d5-e5bec0441795"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Ac, id sifioneaithi:\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "Mine....\n",
            "I'ssc'me I'seveeo fo l o han Yurbones chan, ans he!! Yu:\n",
            "I chonosugow. Werikaing o, ineareathommp ued.\n",
            "Rigo:\n",
            "Yusasue p....\n",
            "Yuswawato pal w?\n",
            "\n",
            "Wour:\n",
            "Sorathe hat epr t y w you:\n",
            "\n",
            "\n",
            "Mimer h, cr w Bew ugiditr, tiok t thol:\n",
            "Rith-no:\n",
            "Rinare Le airy. Lo tmen! s.\n",
            "Yu't thai:\n",
            "Ts\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=300)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mzedwSah988C"
      },
      "source": [
        "# Now for the Transformer Fundamentals!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MlxFojAr-Cor"
      },
      "source": [
        "## The mathematical trick to self-attention: triangular matrices for weighted averages"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yR0CbrSV0Eqk",
        "outputId": "81e17413-92c8-4199-d06e-82eb9cff3193"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([2, 8, 2])\n",
            "tensor([[[ 0.1808, -0.0700],\n",
            "         [-0.3596, -0.9152],\n",
            "         [ 0.6258,  0.0255],\n",
            "         [ 0.9545,  0.0643],\n",
            "         [ 0.3612,  1.1679],\n",
            "         [-1.3499, -0.5102],\n",
            "         [ 0.2360, -0.2398],\n",
            "         [-0.9211,  1.5433]],\n",
            "\n",
            "        [[ 1.3488, -0.1396],\n",
            "         [ 0.2858,  0.9651],\n",
            "         [-2.0371,  0.4931],\n",
            "         [ 1.4870,  0.5910],\n",
            "         [ 0.1260, -1.5627],\n",
            "         [-1.1601, -0.3348],\n",
            "         [ 0.4478, -0.8016],\n",
            "         [ 1.5236,  2.5086]]])\n"
          ]
        }
      ],
      "source": [
        "# consider the following toy example:\n",
        "\n",
        "torch.manual_seed(1337)\n",
        "B,T,C = 2,8,2 # batch, time, channels\n",
        "x = torch.randn(B,T,C)\n",
        "print(x.shape)\n",
        "print(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ltEwVCuxIegD",
        "outputId": "ff4ff464-1bee-4263-9fd7-b4fd620140a7"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]]])"
            ]
          },
          "execution_count": 66,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# We want x[b,t] = mean_{i<=t} x[b,i] to very badly encode info of tokens before token t\n",
        "xbow = torch.zeros((B,T,C))\n",
        "for b in range(B):\n",
        "    for t in range(T):\n",
        "        xprev = x[b,:t+1] # (t,C)\n",
        "        xbow[b,t] = torch.mean(xprev, 0)\n",
        "xbow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wyHfiV0OI2EY",
        "outputId": "5334dd5a-7014-441e-c4ed-f25161ed21e2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.5000, 0.5000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.3333, 0.3333, 0.3333, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2500, 0.2500, 0.2500, 0.2500, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.2000, 0.2000, 0.2000, 0.2000, 0.2000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.1667, 0.0000, 0.0000],\n",
            "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.0000],\n",
            "        [0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250, 0.1250]])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]]])"
            ]
          },
          "execution_count": 67,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# better way to do this: triangular matrix!\n",
        "# version 2: using matrix multiply for a weighted aggregation\n",
        "wei = torch.tril(torch.ones(T, T))\n",
        "wei = wei / wei.sum(1, keepdim=True)\n",
        "print(wei)\n",
        "xbow2 = wei @ x # (B, T, T) @ (B, T, C) ----> (B, T, C)\n",
        "xbow2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XBLSxfkAJD0A",
        "outputId": "add82619-0dc9-43e1-9fff-e1f48e45a767"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "tensor([[[ 0.1808, -0.0700],\n",
              "         [-0.0894, -0.4926],\n",
              "         [ 0.1490, -0.3199],\n",
              "         [ 0.3504, -0.2238],\n",
              "         [ 0.3525,  0.0545],\n",
              "         [ 0.0688, -0.0396],\n",
              "         [ 0.0927, -0.0682],\n",
              "         [-0.0341,  0.1332]],\n",
              "\n",
              "        [[ 1.3488, -0.1396],\n",
              "         [ 0.8173,  0.4127],\n",
              "         [-0.1342,  0.4395],\n",
              "         [ 0.2711,  0.4774],\n",
              "         [ 0.2421,  0.0694],\n",
              "         [ 0.0084,  0.0020],\n",
              "         [ 0.0712, -0.1128],\n",
              "         [ 0.2527,  0.2149]]])"
            ]
          },
          "execution_count": 68,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# even better: softmax for normalization of weights\n",
        "# version 3: use Softmax\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = torch.zeros((T,T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "xbow3 = wei @ x\n",
        "xbow3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yvymyaOiJbbo",
        "outputId": "d2f3783f-74c2-45d0-91b0-ee05c9b59062"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4700, 0.5300, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.3140, 0.3170, 0.3690, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2060, 0.2090, 0.2640, 0.3210, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1920, 0.1650, 0.2050, 0.2140, 0.2250, 0.0000, 0.0000, 0.0000],\n",
            "         [0.1260, 0.1320, 0.0900, 0.0690, 0.2020, 0.3810, 0.0000, 0.0000],\n",
            "         [0.1440, 0.1500, 0.1540, 0.1630, 0.1220, 0.1160, 0.1500, 0.0000],\n",
            "         [0.0580, 0.0460, 0.0440, 0.0340, 0.1330, 0.1390, 0.0480, 0.4990]],\n",
            "\n",
            "        [[1.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.4970, 0.5030, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0040, 0.0540, 0.9430, 0.0000, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.5030, 0.1000, 0.0140, 0.3840, 0.0000, 0.0000, 0.0000, 0.0000],\n",
            "         [0.2570, 0.1220, 0.0820, 0.1950, 0.3440, 0.0000, 0.0000, 0.0000],\n",
            "         [0.0330, 0.1210, 0.5640, 0.0410, 0.0430, 0.1990, 0.0000, 0.0000],\n",
            "         [0.2080, 0.0880, 0.0420, 0.1640, 0.2250, 0.0830, 0.1890, 0.0000],\n",
            "         [0.2230, 0.0870, 0.0160, 0.2280, 0.1030, 0.0330, 0.1210, 0.1890]]],\n",
            "       grad_fn=<RoundBackward1>)\n",
            "tensor([[[ 5.4000e-02, -1.3900e-01, -1.7200e-01, -1.0700e-01,  3.0000e-03,\n",
            "          -9.0000e-03, -5.4000e-02,  2.1000e-02,  1.2100e-01, -6.7000e-02,\n",
            "          -4.0000e-02,  6.1000e-02, -3.1000e-02, -1.2000e-01,  1.1400e-01,\n",
            "          -1.8000e-02],\n",
            "         [-1.1800e-01, -1.2300e-01, -2.7100e-01, -2.2000e-01, -1.4000e-02,\n",
            "           2.7900e-01,  1.9300e-01, -2.1100e-01,  2.9400e-01, -2.4800e-01,\n",
            "           3.0400e-01, -2.0300e-01,  2.1900e-01,  2.8000e-02,  2.5200e-01,\n",
            "          -2.3500e-01],\n",
            "         [ 1.6000e-02, -2.2000e-01, -3.2200e-01, -2.2200e-01, -2.0000e-03,\n",
            "           1.0700e-01,  1.6000e-02, -6.3000e-02,  2.7000e-01, -1.8500e-01,\n",
            "           8.0000e-02, -1.2000e-02,  5.5000e-02, -1.3300e-01,  2.4400e-01,\n",
            "          -1.2100e-01],\n",
            "         [ 1.2900e-01, -3.3800e-01, -4.2000e-01, -2.6100e-01,  6.0000e-03,\n",
            "          -1.6000e-02, -1.2700e-01,  4.8000e-02,  2.9700e-01, -1.6500e-01,\n",
            "          -9.1000e-02,  1.4300e-01, -7.1000e-02, -2.8900e-01,  2.7900e-01,\n",
            "          -4.9000e-02],\n",
            "         [ 1.5800e-01, -2.0200e-01, -1.9300e-01, -9.4000e-02,  1.2000e-02,\n",
            "          -1.5000e-01, -1.9400e-01,  1.4100e-01,  8.6000e-02, -6.0000e-03,\n",
            "          -2.2100e-01,  2.1100e-01, -1.6300e-01, -2.3800e-01,  9.2000e-02,\n",
            "           7.8000e-02],\n",
            "         [-1.3700e-01,  1.8700e-01,  1.8500e-01,  9.5000e-02, -1.0000e-02,\n",
            "           1.2200e-01,  1.6500e-01, -1.1700e-01, -9.1000e-02,  1.7000e-02,\n",
            "           1.8400e-01, -1.8000e-01,  1.3700e-01,  2.1200e-01, -9.4000e-02,\n",
            "          -5.9000e-02],\n",
            "         [ 3.9000e-02, -1.2400e-01, -1.6000e-01, -1.0200e-01,  2.0000e-03,\n",
            "           7.0000e-03, -3.5000e-02,  7.0000e-03,  1.1800e-01, -6.9000e-02,\n",
            "          -1.8000e-02,  4.0000e-02, -1.5000e-02, -1.0000e-01,  1.0900e-01,\n",
            "          -2.8000e-02],\n",
            "         [-6.8000e-02,  6.2800e-01,  9.0300e-01,  6.1500e-01,  5.0000e-03,\n",
            "          -2.6500e-01, -1.3000e-02,  1.4800e-01, -7.4500e-01,  5.0200e-01,\n",
            "          -1.8100e-01, -1.0000e-03, -1.2200e-01,  3.9900e-01, -6.7500e-01,\n",
            "           3.1500e-01]],\n",
            "\n",
            "        [[ 4.6400e-01, -8.9900e-01, -1.0320e+00, -6.0300e-01,  2.9000e-02,\n",
            "          -2.5300e-01, -5.1400e-01,  2.9500e-01,  6.5500e-01, -3.0200e-01,\n",
            "          -4.9100e-01,  5.6700e-01, -3.7000e-01, -8.6600e-01,  6.3200e-01,\n",
            "           3.0000e-02],\n",
            "         [ 3.5800e-01, -3.6000e-01, -2.9100e-01, -1.1100e-01,  2.9000e-02,\n",
            "          -3.9800e-01, -4.5500e-01,  3.5600e-01,  6.9000e-02,  7.5000e-02,\n",
            "          -5.4800e-01,  4.9200e-01, -4.0300e-01, -4.8500e-01,  9.6000e-02,\n",
            "           2.3800e-01],\n",
            "         [-6.0400e-01,  1.3840e+00,  1.6660e+00,  1.0110e+00, -3.4000e-02,\n",
            "           1.9900e-01,  6.3200e-01, -3.0200e-01, -1.1310e+00,  5.8900e-01,\n",
            "           5.3200e-01, -7.0300e-01,  4.0700e-01,  1.2440e+00, -1.0730e+00,\n",
            "           9.8000e-02],\n",
            "         [ 4.9000e-01, -6.9000e-01, -6.9600e-01, -3.6200e-01,  3.5000e-02,\n",
            "          -4.2500e-01, -5.8800e-01,  4.1100e-01,  3.5200e-01, -7.9000e-02,\n",
            "          -6.5000e-01,  6.4100e-01, -4.8300e-01, -7.7200e-01,  3.6300e-01,\n",
            "           1.9900e-01],\n",
            "         [ 1.5100e-01, -4.5500e-01, -5.8200e-01, -3.6900e-01,  7.0000e-03,\n",
            "           1.6000e-02, -1.3900e-01,  3.4000e-02,  4.2500e-01, -2.4800e-01,\n",
            "          -7.8000e-02,  1.5800e-01, -6.3000e-02, -3.7100e-01,  3.9600e-01,\n",
            "          -9.5000e-02],\n",
            "         [-4.0100e-01,  8.8000e-01,  1.0470e+00,  6.3000e-01, -2.3000e-02,\n",
            "           1.5600e-01,  4.2600e-01, -2.1600e-01, -7.0000e-01,  3.5500e-01,\n",
            "           3.7300e-01, -4.7400e-01,  2.8400e-01,  8.0500e-01, -6.6600e-01,\n",
            "           4.0000e-02],\n",
            "         [ 1.1700e-01, -4.3200e-01, -5.7200e-01, -3.7000e-01,  4.0000e-03,\n",
            "           6.1000e-02, -9.4000e-02, -5.0000e-03,  4.3400e-01, -2.6500e-01,\n",
            "          -2.0000e-02,  1.0900e-01, -2.1000e-02, -3.3100e-01,  4.0000e-01,\n",
            "          -1.2400e-01],\n",
            "         [ 4.0400e-01, -4.5100e-01, -3.9500e-01, -1.7300e-01,  3.1000e-02,\n",
            "          -4.2200e-01, -5.0600e-01,  3.8500e-01,  1.3600e-01,  4.4000e-02,\n",
            "          -5.9600e-01,  5.4900e-01, -4.4000e-01, -5.7200e-01,  1.6000e-01,\n",
            "           2.4000e-01]]], grad_fn=<RoundBackward1>)\n"
          ]
        }
      ],
      "source": [
        "# finally: Query (what i look for), Key (What i am in this), \n",
        "# Value (My private value, embedded) for self-attention\n",
        "# version 4: single head self-attention\n",
        "head_size = 16\n",
        "query = nn.Linear(C, head_size, bias=False) # Linear layer C (embed) -> head size (16)\n",
        "key = nn.Linear(C, head_size, bias=False) # Linear layer C (embed) -> head size (16)\n",
        "value = nn.Linear(C, head_size, bias=False) # Linear layer C (embed) -> head size (16)\n",
        "\n",
        "q = query(x) # (B,T,16)\n",
        "k = key(x) # (B,T,16)\n",
        "wei = q @ k.transpose(-2, -1) # (B,T,16) @ (B,16,T) -> (B,T,T)\n",
        "\n",
        "wei = wei * C**-0.5 # scaled attention as to not sharpen softmax\n",
        "\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "wei = wei.masked_fill(tril == 0, float('-inf'))\n",
        "wei = F.softmax(wei, dim=-1)\n",
        "print(torch.round(wei, decimals=3))\n",
        "\n",
        "v = value(x)\n",
        "out = wei @ v\n",
        "print(torch.round(out, decimals=3))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z9UBFTl7RJz6"
      },
      "source": [
        "# Time to put attention in our last model!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "id": "29tuDoYfRInY"
      },
      "outputs": [],
      "source": [
        "class SelfAttentionHead(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "    def __init__(self, n_embd, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "    def forward(self, x):\n",
        "        B,T,C = x.shape\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        k = self.key(x)   # (B,T,C)\n",
        "\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        wei = wei * C**-0.5 # scaled attention\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,C)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "id": "yWMdfaSDSfqm"
      },
      "outputs": [],
      "source": [
        "class BigramEmbedAttentionLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, embed_size):\n",
        "        super().__init__()\n",
        "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        # embed block sized context length as positional embeddings of the same size\n",
        "        self.position_embedding_table = nn.Embedding(block_size, embed_size)\n",
        "        # Language Modelling (?) Head is a standard linear layer to go from \n",
        "        # embeddings back to logits of vocab_size\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "        # single head self-attention\n",
        "        self.sa_head = SelfAttentionHead(embed_size, embed_size)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T)) # (T,C) [0...T-1]\n",
        "        x = tok_embd + pos_embd\n",
        "        # apply self-attention\n",
        "        x = self.sa_head(x)\n",
        "        # get logits with linear layer\n",
        "        logits = self.lm_head(x) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            #crop idx to the last block_size tokens\n",
        "            idx_context = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_context)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cLy8v6ipS-ih",
        "outputId": "9731dd14-63cb-471f-f2c4-312f5311906c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning step: 0\n",
            "learning step: 5000\n",
            "2.2536041736602783\n"
          ]
        }
      ],
      "source": [
        "# training!\n",
        "m = BigramEmbedAttentionLanguageModel(vocab_size, 32)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "batch_size = 32\n",
        "for steps in range(10000):  # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    # backprop\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (steps % 5000 == 0):\n",
        "        print(\"learning step:\", steps)\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 73,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xeF8Io5BTyvt",
        "outputId": "32fb8015-a9b6-41f7-f93d-d629ed98016d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Mio-su mugh at's and, wa hesayorres derkontere ter dn rewh!\n",
            "Oh?\n",
            "\n",
            "Ritt an.\n",
            "\n",
            "Wer, id ogise aks mad et thave dee..? Ohaw, thetlanygingi.\n",
            "\n",
            "Mioko:\n",
            "I'mter aicaka tot'r ero sothiond rd itel, Leeat dor it ctoun'tsu:\n",
            "\n",
            "Sua sakicu, thikai wes whet avin oll y-add I cavinofi do din ounckead soo w. You, ihtiss!\n",
            "\n",
            "Ritndo ifalki:\n",
            "Lethel pled ole tin hes, yoke thare lut?\n",
            "\n",
            "Sayo ouritosu thaculig a tropat'l a tis t awor be at whe alclou'red Cmon toloure th wa!\n",
            "\n",
            "Yuifrik! is.\n",
            "\n",
            "Miits mt.\n",
            "\n",
            "I' mmonbet pger lbare't theal\n"
          ]
        }
      ],
      "source": [
        "print(decode(m.generate(idx=torch.zeros((1, 1), dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SkREJPLwVCo3"
      },
      "source": [
        "# More heads! Multi-Head Attention"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "2Rgkap80VCLY"
      },
      "outputs": [],
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, n_embd, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([SelfAttentionHead(n_embd, head_size) for _ in range(num_heads)])\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1) # concat single-head results\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 75,
      "metadata": {
        "id": "r9RFKTHiVlJh"
      },
      "outputs": [],
      "source": [
        "class BigramEmbedMultiHeadAttentionLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, embed_size, head_num):\n",
        "        super().__init__()\n",
        "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        # embed block sized context length as positional embeddings of the same size\n",
        "        self.position_embedding_table = nn.Embedding(block_size, embed_size)\n",
        "        # Language Modelling (?) Head is a standard linear layer to go from \n",
        "        # embeddings back to logits of vocab_size\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "        # multi-head self-attention\n",
        "        self.sa_heads = MultiHeadAttention(head_num, embed_size, embed_size//head_num) \n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T)) # (T,C) [0...T-1]\n",
        "        x = tok_embd + pos_embd\n",
        "        # apply multi-head self-attention\n",
        "        x = self.sa_heads(x)\n",
        "        # get logits with linear layer\n",
        "        logits = self.lm_head(x) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            #crop idx to the last block_size tokens\n",
        "            idx_context = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_context)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 76,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "93ohtB_CWstI",
        "outputId": "94ee0176-a814-4ce4-91e0-a977275f47f6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning step: 0\n",
            "learning step: 5000\n",
            "2.039285659790039\n"
          ]
        }
      ],
      "source": [
        "# training!\n",
        "m = BigramEmbedMultiHeadAttentionLanguageModel(vocab_size, 16, 32, 8)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "batch_size = 32\n",
        "for steps in range(10000):  # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    # backprop\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (steps % 5000 == 0):\n",
        "        print(\"learning step:\", steps)\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 77,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J7ptYRjZW5Id",
        "outputId": "f4374707-b7b6-4964-b728-d4c2df8bf820"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[28, 81, 76, 74, 56, 25,  0]])\n",
            "Azusa:\n",
            "He onstur thindy that ffere!\n",
            "\n",
            "Ritsu:\n",
            "\"St I'll we idrsuyso thins of coe!.. I now ps oonyou're inden'ce to as fore?\n",
            "Azin:\n",
            "Yeach, navery! Tlub so reanmally ondo this kicke hou for pome crote, is aninging to ck!\n",
            "\n",
            "Nod, hav ine fore so a goots mup you wap?\n",
            "It yis you U!\n",
            "\n",
            "Tsumunffed rony fory alroe're the're fornd ithinku:\n",
            "No fit tammet geaf!\n",
            "Ondow. li-. Mio the You maryment.\n",
            "\n",
            "Jun:\n",
            "Therymight lat boudunt. Mum?\n",
            "\n",
            "Mio:\n",
            "Welle othe there epla funcer thing dof bof sore.\n",
            "\n",
            "Mio:\n",
            "Re? Ryou's & Mio:\n",
            "Yuh!\n",
            "\n",
            "Azusa:\n",
            "W\n"
          ]
        }
      ],
      "source": [
        "idx = encode(\"Azusa:\\n\")\n",
        "print(torch.tensor([idx]))\n",
        "print(decode(m.generate(idx=torch.tensor([idx], dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJNncM0IXlSz"
      },
      "source": [
        "# Time to think: Feed-Forward to compute attention results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 78,
      "metadata": {
        "id": "bLFvUg0dXhVS"
      },
      "outputs": [],
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, n_embed, n_hidden):\n",
        "        super().__init__()\n",
        "        self.lin_1 = nn.Linear(n_embed, n_hidden)\n",
        "        self.lin_2 = nn.Linear(n_hidden, n_embed)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.lin_1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.lin_2(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 79,
      "metadata": {
        "id": "57S8adsLU9E8"
      },
      "outputs": [],
      "source": [
        "class BigramEmbedMultiHeadAttentionFeedForwardLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, embed_size, head_num):\n",
        "        super().__init__()\n",
        "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        # embed block sized context length as positional embeddings of the same size\n",
        "        self.position_embedding_table = nn.Embedding(block_size, embed_size)\n",
        "        # Language Modelling (?) Head is a standard linear layer to go from \n",
        "        # embeddings back to logits of vocab_size\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "        # multi-head self-attention\n",
        "        self.sa_heads = MultiHeadAttention(head_num, embed_size, embed_size//head_num) \n",
        "        # feed forward\n",
        "        self.ff_layer = FeedForward(embed_size, 128)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T)) # (T,C) [0...T-1]\n",
        "        x = tok_embd + pos_embd\n",
        "        # apply multi-head self-attention\n",
        "        x = self.sa_heads(x)\n",
        "        # feed forward\n",
        "        x = self.ff_layer(x)\n",
        "        # get logits with linear layer\n",
        "        logits = self.lm_head(x) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            #crop idx to the last block_size tokens\n",
        "            idx_context = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_context)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 80,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "miKKKTsxVZqo",
        "outputId": "4508cb7f-f94e-44c0-ddee-791d71789dbd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning step: 0\n",
            "learning step: 5000\n",
            "1.819643259048462\n"
          ]
        }
      ],
      "source": [
        "# training!\n",
        "m = BigramEmbedMultiHeadAttentionFeedForwardLanguageModel(vocab_size, 16, 32, 8)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "batch_size = 32\n",
        "for steps in range(10000):  # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    # backprop\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (steps % 5000 == 0):\n",
        "        print(\"learning step:\", steps)\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 81,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7H3ehdNzXS6p",
        "outputId": "b102ebed-9284-4fa4-c7f7-ce5bd2f43b63"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[28, 81, 76, 74, 56, 25,  0]])\n",
            "Azusa:\n",
            "Oh, Mio:\n",
            "Gien in is hen?\n",
            "\n",
            "\n",
            "Yui:\n",
            "Ejugi?\n",
            "\n",
            "Nona os!\n",
            "\n",
            "Azusa:\n",
            "Oh!\n",
            "\n",
            "Yui:\n",
            "Et to of thing brould banwako:\n",
            "Um…! Ritcuckeentn..\n",
            "\n",
            "W..\n",
            "\n",
            "Mio:\n",
            "Pverer a shand to ongethink slay!\n",
            "\n",
            "Ritsu:\n",
            "Mugi,.\n",
            "\n",
            "Ummo:..\n",
            "\n",
            "That a like we driing! Pup ore.\n",
            "\n",
            "Yui:\n",
            "Sould thing onk thy said see you the doI're fict mop fel.\n",
            "\n",
            "Sawako:\n",
            "Oh!\n",
            "\n",
            "Ui:\n",
            "That's shinks]\n",
            "\n",
            "Yui:\n",
            "Mm. I scher mant reed.\n",
            "\n",
            "Y-chan to lass it's light?\n",
            "We like clased it.\n",
            "\n",
            "Azusa:\n",
            "D?\n",
            "Os.\n",
            "\n",
            "Ritcht.\n",
            "\n",
            "Mio:\n",
            "How, that be lisht!\n",
            "\n",
            "Ritsu:\n",
            "Wes and.\n",
            "\n",
            "Ange.\n",
            "\n",
            "Cle you wough?\n",
            "\n",
            "Azusa:\n",
            "You got t\n"
          ]
        }
      ],
      "source": [
        "idx = encode(\"Azusa:\\n\")\n",
        "print(torch.tensor([idx]))\n",
        "print(decode(m.generate(idx=torch.tensor([idx], dtype=torch.long), max_new_tokens=500)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 82,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gs38UxKCX2Ng",
        "outputId": "04dfd765-2007-46d6-ab46-e4d94a8d899d"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "17981"
            ]
          },
          "execution_count": 82,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "total_params"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qokuZrUSYZ1E"
      },
      "source": [
        "# Make it scalable: repeatable Blocks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 83,
      "metadata": {
        "id": "-LZ1yfLdYZaI"
      },
      "outputs": [],
      "source": [
        "class Block(nn.Module):\n",
        "    def __init__(self, n_heads, n_embd):\n",
        "        super().__init__()\n",
        "        self.sa_heads = MultiHeadAttention(n_heads, n_embd, n_embd//n_heads)\n",
        "        self.ff_layer = FeedForward(n_embd, 128)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.sa_heads(x)\n",
        "        x = self.ff_layer(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 84,
      "metadata": {
        "id": "k6cA2WbrayyL"
      },
      "outputs": [],
      "source": [
        "class TransformerNoResidualNoNormModel(nn.Module):\n",
        "\n",
        "    def __init__(self, vocab_size, block_size, embed_size, head_num, layer_num):\n",
        "        super().__init__()\n",
        "        # embed raw tokens to a lower dimensional embedding with embed_size\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, embed_size)\n",
        "        # embed block sized context length as positional embeddings of the same size\n",
        "        self.position_embedding_table = nn.Embedding(block_size, embed_size)\n",
        "        # Language Modelling (?) Head is a standard linear layer to go from \n",
        "        # embeddings back to logits of vocab_size\n",
        "        self.lm_head = nn.Linear(embed_size, vocab_size)\n",
        "        # transformer blocks\n",
        "        self.blocks = nn.Sequential(*[Block(head_num, embed_size) for _ in range(layer_num)])\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_embd = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_embd = self.position_embedding_table(torch.arange(T)) # (T,C) [0...T-1]\n",
        "        x = tok_embd + pos_embd\n",
        "        # go through blocks\n",
        "        x = self.blocks(x)\n",
        "        # get logits with linear layer\n",
        "        logits = self.lm_head(x) # (B,T,C)\n",
        "        \n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "    \n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            #crop idx to the last block_size tokens\n",
        "            idx_context = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_context)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9F21-gK2bv2O",
        "outputId": "b3935e45-bb30-40e3-d7f5-16b3f1f339d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "learning step: 0\n",
            "learning step: 5000\n",
            "1.7578377723693848\n"
          ]
        }
      ],
      "source": [
        "# training!\n",
        "m = TransformerNoResidualNoNormModel(vocab_size, 32, 64, 8, 4)\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=1e-3)\n",
        "batch_size = 32\n",
        "for steps in range(10000):  # increase number of steps for good results...\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb)\n",
        "\n",
        "    # backprop\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (steps % 5000 == 0):\n",
        "        print(\"learning step:\", steps)\n",
        "\n",
        "print(loss.item())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 86,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "00r0pbm3b5eX",
        "outputId": "bdd3b34e-32a0-4724-b528-c162c0fd0c3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[52, 76, 64, 25,  0]])\n",
            "Yui:\n",
            "Spodent fust not Ritsu?\n",
            "\n",
            "Ritsu:\n",
            "Oh, even carlyan Mingleably is much o.\n",
            "\n",
            "Azusa:\n",
            "Wah! Gond to could is freachub came our go that yoori curner.\n",
            "\n",
            "Yui:\n",
            "Ya Far tw oke to heryser in at.\n",
            "\n",
            "Yui:\n",
            "Ah! Yexal wor that wait mend\"\n",
            "\n",
            "Ritsu:\n",
            "But jobk all at wemeret ould it knar!\n",
            "\n",
            "Yui:\n",
            "Or iAswtel.\n",
            "Nokay what hereters she. Jazme our memet! Worger weryted foor stodor! Sonding is rike to? You ady. But teme that to mais one.\n",
            "\n",
            "Ui:\n",
            "Ritsmaka’ing??\n",
            "\n",
            "Azusa:\n",
            "(Oh, that, whuh! And this this!\n",
            "\n",
            "Mio:\n",
            "moil have flet?\n",
            "\n",
            "Yui:\n",
            "You sis Hecuse onitarnd at comeeting hamal, it's yoursts modly hean coult, them.\n",
            "\n",
            "Yui:\n",
            "Hey, this's make the votes it riser sentbennus..\n",
            "\n",
            "Yui:\n",
            "CuUby, an ure, andle oursped it you bwor all home is sayure tooudet balways, I my we sor to Azuzm.\n",
            "\n",
            "Yui:\n",
            "Jto, I'm it renuthing. Thatly, So.\n",
            "\n",
            "Ritsu:\n",
            "Hm...\n",
            "\n",
            "Yui:\n",
            "Graidy, Ui day eut you this your wean wis you tidled sle is me. I whuh! I just bunt aust!\n",
            "\n",
            "Ui:\n",
            "It's am wegposer goids sterping srepmoct over in't sonyod you in ope?\n",
            "\n",
            "Mio:\n",
            "Yes thoor is shanter jar tevites d\n"
          ]
        }
      ],
      "source": [
        "idx = encode(\"Yui:\\n\")\n",
        "print(torch.tensor([idx]))\n",
        "print(decode(m.generate(idx=torch.tensor([idx], dtype=torch.long), max_new_tokens=1000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-sKFJ8u4b8h6",
        "outputId": "29c477c2-e474-4000-8056-1a00e05354a3"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "129501"
            ]
          },
          "execution_count": 87,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "total_params = sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "total_params"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
